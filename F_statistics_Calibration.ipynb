{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "246ec3ba",
   "metadata": {},
   "source": [
    "# F-statistic Calibration\n",
    "## Introduction\n",
    "Distinguishing flat dose-response curves (i.e., completly inactive/active) is essential prior perfroming any cruve fitting. If flat curves are not distinguished from the true sigmoidal curves, it can lead to force fitting of sigmoidal models, which may produce low residual sum of square and be mathematically sound, but miss the messiness of real biological experiemnts, assay noise and biological meanings. Thus, it would be essential to statistically examine wether expeiremntal dose-response curves are signtifacntly different from a flat-line.\n",
    "\n",
    "To address this issue, Bayer et al. (2023)[1], used simulations to re-calibrate the F-statistics for non-linear sigmoidal dose-response curves. Here, we adopted a similar appraoch to simulate null model and recalibrate F-statistics.\n",
    "\n",
    "## Implementation\n",
    "- **Models**:\n",
    "    - 1-parameter model: The flat model assumes no dose effects and it would be a constant across all doses.\n",
    "    - Two-parameter logistic model: in this model, given that we are using normalized dose-response cruves clipped between 0 and 1, we fixed the front and back plateau at 0 and 1, and kept the slope (B) and midpoint (C) to be adjustable.\n",
    "- **Residual sum of sqaures (RSS)**: to measure how much the model's prediction miss the actual data points, we calculate the RSS as follows:\n",
    "    - $\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$; where $ y_i $ is the observed (actual) data point for the $ i $-th observation, the $ \\hat{y}_i $: is the predicted value from the model for the $ i $-th observation and $ n $: is the number of doses points (10; 128-0.25 µg/mL).\n",
    "- **F-statistics**: to see if the more complex model (2-PL) exaplins the data better, we calculated the F-statistics as follows:\n",
    "    - $F = \\left( \\frac{\\text{RSS}_0 - \\text{RSS}_1}{\\text{RSS}_1} \\right) \\times (n - k)$; where $ \\text{RSS}_0 $ is the RSS of the 1-parameter model, $ \\text {RSS}_1 $ is the RSS of the 2-PL model, $ n $ number of doses and $ k $ is the number of parameters in the 2-PL models which is equal to 2.\n",
    "        - Note: if the 2-PL fit fails or produces a higher RSS, it defaults to RSS0 to avoid invalid F values.\n",
    "- **Curve-Fitting**: to find the best fit and prevent local minima, the 2-PL model is fitted using up to 7 different starting points to minimize the residual sum of squares (RSS) using the Trust Region Reflective (TRF) method and further minimzed using Nelder-Mead method.\n",
    "    - The initial starting point is B = 1 and C = median concentration dose (i.e., 6).\n",
    "    - For the remaining starting points, we use normally picked values from the bounds : B [-10, 10] and [Lowest Concentration/4 ,Highest Concentration × 4]\n",
    "    - For each start, we use \"scipy.optimize.curve_fit\" with the TRF method, alllowing up to 8000 function evaluation.\n",
    "    - The best parameters then will be further refined using scipy.optimize.minimize using Nelder-Mead method up to 20,000 evaluations.\n",
    "        - Note: if fits was not succeeds, the simulation deafults to flat RSS.\n",
    "- **Monte Carlo Simulation**: to calibrate the F-statistic under the null hypothesis (flat-curve), we perform 100,000 simulations per control type (Growth or Sterility) per organism.\n",
    "    - Control Mean and SD Pools: we computed the mean and standard deviation of each growth and sterilty control samples, while discarding those with SD = 0 or missing data.\n",
    "    - Simulation: using the pool of  mean and SD, we computed the simulated response data for each of 10 doses as follows:\n",
    "        - $y_i = \\mu + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma)$; where $ y_i $ represents the response at dose $ i $ (for $ i = 1, \\dots, 10 $), $ \\mu $ represents sampled mean from control data, $ \\epsilon_i $: is random noise from a normal distribution with mean 0 and standard deviation $ \\sigma $ (sampled from control data) and $ \\mathcal{N}(0, \\sigma) $ represent the normal distribution with mean 0 and standard deviation $ \\sigma $\n",
    "    - Subsequently, the RSS for flat and 2-PL models are obtained and F statistics is calcauted.\n",
    "- **Distribution Fitting for Calibration:**: following collection of simulated F values:\n",
    "    - Filtered the finite F values and compute a density histogram with bin size of 600\n",
    "    - Used scipy.optimize.least_squares to minimze the residual between the histogram and the scaled F probability density function, where paramters are df1 (starting ~2), df2 (>1), loc, and scale.\n",
    "- **Outputs**: results are stored in a dictionary by organism and control, including the fitted parameters, and pool sizes.\n",
    "\n",
    "## References\n",
    "[1] Florian P. Bayer, Manuel Gander, Bernhard Kuster & Matthew The (2023). CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose–response curves. *Nature Communications*, 14, Article 7902. https://doi.org/10.1038/s41467-023-43696-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b4aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "simulate_null_f_per_org.py – v8.8  (flat vs 2‑PL only + rich runtime metadata)\n",
    "\n",
    "DescriptionÅ\n",
    "-----------\n",
    "Null model:\n",
    "    flat (k = 1)  vs  2‑parameter logistic (k = 2, A = 0, D = 1)\n",
    "\n",
    "• Performs Monte‑Carlo calibration for the F‑test family \"flat_vs_2pl\".\n",
    "• Records df₁*, df₂*, loc, scale per control class (Growth, Sterility).\n",
    "• Writes results to `null_f_params_full.json`.\n",
    "• Captures runtime metadata (wall‑clock time, CPU usage, RSS, etc.)\n",
    "  under a top‑level \"_meta\" key in the same JSON file.\n",
    "\"\"\"\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 0 ▸ imports & constants\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "import os, json, math, warnings, multiprocessing as mp\n",
    "import time, datetime, resource\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from   scipy.optimize import curve_fit, minimize, least_squares\n",
    "from   scipy.stats    import f as f_dist\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from datetime import timezone\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "CSV        = \"/projects/amp/asalehi/Dose/data/clean/full_clean_hts.csv\"\n",
    "CTRL_NAMES = {\"growth\": \"Growth Control\",\n",
    "              \"sterility\": \"Sterility Control\"}\n",
    "\n",
    "ABS_ORDER  = [\"128\",\"64\",\"32\",\"16\",\"8\",\"4\",\"2\",\"1\",\"0.5\",\"0.25\"]\n",
    "IF_COLS    = [f\"i{c}_win\" for c in ABS_ORDER]\n",
    "\n",
    "CONC    = np.array([128,64,32,16,8,4,2,1,0.5,0.25], float)\n",
    "N_DOSES = CONC.size\n",
    "\n",
    "N_SIM_NULL   = 100_000                 # simulations *per* control\n",
    "TOTAL_CPUS   = mp.cpu_count()\n",
    "N_WORKERS    = min(128, TOTAL_CPUS, N_SIM_NULL)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 1 ▸ helper functions\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "flat  = lambda x, c: np.full_like(x, c)\n",
    "_log2 = lambda z: np.log2(np.maximum(z, 1e-12))\n",
    "\n",
    "def two_pl(x, B, C):                    # A = 0, D = 1\n",
    "    return 1.0 / (1 + 2**(B * (_log2(C) - _log2(x))))\n",
    "\n",
    "rss  = lambda y, yhat: np.sum((y - yhat) ** 2)\n",
    "f_cc = lambda r0, r1, n, k: ((r0 - r1) / r1) * (n - k)  # df_num=1 implicitly\n",
    "\n",
    "def fit_best(func, x, y, p0, bounds, n_start=6, *, rng=None):\n",
    "    rng = rng or np.random.default_rng()\n",
    "    best_rss, best_theta = math.inf, None\n",
    "    lo, hi = bounds\n",
    "    starts = [p0] + [rng.uniform(lo, hi) for _ in range(n_start)]\n",
    "\n",
    "    for start in starts:\n",
    "        try:\n",
    "            theta, _ = curve_fit(func, x, y, p0=start, bounds=bounds,\n",
    "                                 method=\"trf\", max_nfev=8000)\n",
    "            r = rss(y, func(x, *theta))\n",
    "            if r < best_rss:\n",
    "                best_rss, best_theta = r, theta\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if best_theta is not None:\n",
    "        res = minimize(lambda t: rss(y, func(x, *t)), best_theta,\n",
    "                       method=\"Nelder-Mead\",\n",
    "                       options={\"maxfev\": 20_000, \"xatol\": 1e-8, \"fatol\": 1e-8})\n",
    "        if res.success and rss(y, func(x, *res.x)) < best_rss:\n",
    "            best_theta, best_rss = res.x, rss(y, func(x, *res.x))\n",
    "    return best_theta, best_rss\n",
    "\n",
    "def df_effective(F_vals, k):\n",
    "    \"\"\"four‑parameter non‑central F fit\"\"\"\n",
    "    F_vals = F_vals[np.isfinite(F_vals)]\n",
    "    if F_vals.size == 0:\n",
    "        raise RuntimeError(\"All simulated F values were NaN/Inf.\")\n",
    "\n",
    "    hist, edges = np.histogram(F_vals, bins=600, density=True)\n",
    "    centres     = 0.5 * (edges[1:] + edges[:-1])\n",
    "\n",
    "    def resid(p):\n",
    "        df1, df2, loc, scale = p\n",
    "        z = (centres - loc) / scale\n",
    "        return f_dist.pdf(z, df1, df2) / scale - hist\n",
    "\n",
    "    p0 = [k, max(k, 2), 0.1, 1.0]\n",
    "    lb = [2, 1, -np.inf, 0.1]\n",
    "    ub = [1e6, 1e6,  np.inf, 10]\n",
    "    sol = least_squares(resid, p0, bounds=(lb, ub), method=\"trf\")\n",
    "    return sol.x  # df1, df2, loc, scale\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 2 ▸ Monte‑Carlo simulation  (single F‑family)\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "def simulate_nulls(seed, n_sim, sigma_pool, mean_pool):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_pools = len(sigma_pool)\n",
    "    indices = rng.integers(0, n_pools, size=n_sim)\n",
    "\n",
    "    F_two = np.empty(n_sim, np.float32)   # flat vs 2‑PL\n",
    "\n",
    "    b_two = ([-10, CONC.min() / 4], [10, CONC.max() * 4])\n",
    "    p0_two = [1.0, np.median(CONC)]\n",
    "\n",
    "    for i in range(n_sim):\n",
    "        idx = indices[i]\n",
    "        base = mean_pool[idx]\n",
    "        sd = sigma_pool[idx]\n",
    "        y  = base + rng.normal(0, sd, N_DOSES)\n",
    "        r0 = rss(y, flat(CONC, y.mean()))\n",
    "        _, r2 = fit_best(two_pl, CONC, y, p0_two, b_two, rng=rng)\n",
    "        r2 = r2 if r2 is not None else r0\n",
    "        F_two[i] = f_cc(r0, r2, N_DOSES, 2)\n",
    "\n",
    "    return F_two\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 3 ▸ calibration wrapper\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "def run_calibration(df_all):\n",
    "    organisms = sorted(df_all['Organism'].unique())\n",
    "    results = {}\n",
    "    for org in organisms:\n",
    "        print(f\"Processing organism: {org}\")\n",
    "        df_org = df_all.query(\"Organism == @org\")\n",
    "        if df_org.empty:\n",
    "            print(f\"No data for {org}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        pools = {}\n",
    "        for lbl, pid in CTRL_NAMES.items():\n",
    "            ctrl_df = df_org.loc[df_org.Peptide_ID.eq(pid), IF_COLS].dropna(how='all')\n",
    "            if ctrl_df.empty:\n",
    "                continue\n",
    "            means = ctrl_df.apply(lambda r: np.nanmean(r), axis=1).to_numpy(float)\n",
    "            sigs = ctrl_df.apply(lambda r: np.nanstd(r, ddof=0), axis=1).to_numpy(float)\n",
    "            mask = sigs > 0\n",
    "            if np.sum(mask) >= 3:\n",
    "                pools[lbl] = {\"mean_pool\": means[mask], \"sigma_pool\": sigs[mask]}\n",
    "\n",
    "        if not pools:\n",
    "            print(f\"No valid controls for {org}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Generate sanity check plots\n",
    "        n_controls = len(pools)\n",
    "        if n_controls > 0:\n",
    "            org_safe = re.sub(r'[^a-zA-Z0-9_-]', '_', org)\n",
    "            fig, axs = plt.subplots(2, n_controls, figsize=(7 * n_controls, 8), dpi=300, sharey='row')\n",
    "            if n_controls == 1:\n",
    "                axs = axs[:, np.newaxis]\n",
    "\n",
    "            color_means = \"#408cbf\"\n",
    "            color_sds = \"#ff9332\"\n",
    "\n",
    "            col_idx = 0\n",
    "            for lbl in sorted(pools.keys()):\n",
    "                mean_data = pools[lbl][\"mean_pool\"]\n",
    "                sigma_data = pools[lbl][\"sigma_pool\"]\n",
    "                axs[0, col_idx].hist(mean_data, bins=20, color=color_means, alpha=0.85, edgecolor=\"none\")\n",
    "                axs[0, col_idx].set_title(f'{lbl.capitalize()} Means', fontsize=12, fontweight='bold')\n",
    "                axs[0, col_idx].set_xlabel('Mean Values', fontsize=10, fontweight='bold', labelpad=8)\n",
    "                axs[0, col_idx].set_ylabel('Frequency', fontsize=10, fontweight='bold', labelpad=8)\n",
    "                axs[1, col_idx].hist(sigma_data, bins=20, color=color_sds, alpha=0.85, edgecolor=\"none\")\n",
    "                axs[1, col_idx].set_title(f'{lbl.capitalize()} SDs', fontsize=12, fontweight='bold')\n",
    "                axs[1, col_idx].set_xlabel('SD Values', fontsize=10, fontweight='bold', labelpad=8)\n",
    "                axs[1, col_idx].set_ylabel('Frequency', fontsize=10, fontweight='bold', labelpad=8)\n",
    "                col_idx += 1\n",
    "\n",
    "            for ax in axs.flatten():\n",
    "                ax.set_facecolor(\"white\")\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f\"{x:.2g}\" if x % 1 != 0 else f\"{x:.0f}\"))\n",
    "                plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "            fig.suptitle(f\"Control Pools Distributions for {org}\", fontweight=\"bold\", fontsize=14, y=0.98)\n",
    "            fig.subplots_adjust(left=0.1, right=0.95, top=0.92, bottom=0.15, wspace=0.25, hspace=0.3)\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95], pad=2.0)\n",
    "            plt.savefig(f'control_pools_hist_{org_safe}.png', dpi=300)\n",
    "            plt.close(fig)\n",
    "            print(f\"✓ saved control_pools_hist_{org_safe}.png\")\n",
    "\n",
    "        results[org] = {}\n",
    "        ctx = mp.get_context(\"fork\")\n",
    "        base, rem = divmod(N_SIM_NULL, N_WORKERS)\n",
    "\n",
    "        for lbl, pool in pools.items():\n",
    "            jobs = [\n",
    "                (137 + 41*i,\n",
    "                 base + (1 if i < rem else 0),\n",
    "                 pool[\"sigma_pool\"],\n",
    "                 pool[\"mean_pool\"])\n",
    "                for i in range(N_WORKERS)\n",
    "            ]\n",
    "            with ctx.Pool(N_WORKERS) as mp_pool:\n",
    "                outs = mp_pool.starmap(simulate_nulls, jobs)\n",
    "\n",
    "            F_TWO = np.concatenate(outs)\n",
    "\n",
    "            results[org][lbl] = {\n",
    "                \"flat_vs_2pl\": dict(\n",
    "                    zip([\"df1\", \"df2\", \"loc\", \"scale\"],\n",
    "                        df_effective(F_TWO, 2))\n",
    "                ),\n",
    "                \"sigma_pool_len\": len(pool[\"sigma_pool\"]),\n",
    "            }\n",
    "\n",
    "    return results\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# 4 ▸ main\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method(\"fork\")\n",
    "    print(f\"→ using {N_WORKERS} workers\")\n",
    "\n",
    "    # ── timers & CPU usage snapshot ───────────────────────────────\n",
    "    t0 = time.perf_counter()\n",
    "    ru0_self = resource.getrusage(resource.RUSAGE_SELF)\n",
    "    ru0_children = resource.getrusage(resource.RUSAGE_CHILDREN)\n",
    "\n",
    "    # ── run main calibration ─────────────────────────────────────\n",
    "    df_all = pd.read_csv(CSV)\n",
    "    results = run_calibration(df_all)\n",
    "\n",
    "    # ── aggregate CPU & time usage ───────────────────────────────\n",
    "    wall_clock = time.perf_counter() - t0\n",
    "    ru1_self = resource.getrusage(resource.RUSAGE_SELF)\n",
    "    ru1_children = resource.getrusage(resource.RUSAGE_CHILDREN)\n",
    "\n",
    "    user_cpu_sec = (ru1_self.ru_utime  - ru0_self.ru_utime)  + \\\n",
    "                   (ru1_children.ru_utime - ru0_children.ru_utime)\n",
    "    sys_cpu_sec  = (ru1_self.ru_stime  - ru0_self.ru_stime)  + \\\n",
    "                   (ru1_children.ru_stime - ru0_children.ru_stime)\n",
    "    max_rss_kb   = max(ru1_self.ru_maxrss, ru1_children.ru_maxrss)\n",
    "\n",
    "    # ── attach metadata & write JSON ─────────────────────────────\n",
    "    results[\"_meta\"] = {\n",
    "        \"timestamp\": datetime.datetime.now(timezone.utc).replace(microsecond=0).isoformat() + \"Z\",\n",
    "        \"script_version\": \"8.8\",\n",
    "        \"n_simulations_per_ctrl\": N_SIM_NULL,\n",
    "        \"n_workers\": N_WORKERS,\n",
    "        \"cpu_count\": TOTAL_CPUS,\n",
    "        \"wall_clock_sec\": round(wall_clock, 3),\n",
    "        \"user_cpu_sec\":   round(user_cpu_sec, 3),\n",
    "        \"sys_cpu_sec\":    round(sys_cpu_sec, 3),\n",
    "        \"max_rss_kb\":     int(max_rss_kb),\n",
    "        \"max_rss_mb\":     round(max_rss_kb / 1024, 1)\n",
    "    }\n",
    "\n",
    "    Path(\"null_f_params_full.json\").write_text(\n",
    "        json.dumps(results, indent=2)\n",
    "    )\n",
    "    print(\"✓ wrote null_f_params_full.json with metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf37b743",
   "metadata": {},
   "source": [
    "## F-statistics Calibration Visual Diagnostics\n",
    "\n",
    "To creat a diagnostic for the evaluation of the F-statistic calibaration, in the following I have simulated random data based the acquired parameters (df1, df2, loc, and scal) obtained from the calibration file to check if the simulated data behaves as expected under a null hypothesis (i.e., flat curve).To do so:\n",
    "\n",
    "1. Pull parameters (df1, df2, loc, and scal) from the JSON for the flat vs2PL model.\n",
    "2. Sample N number of F-values and p-values:\n",
    "    - sample_F(params, n) function generates random F-values\n",
    "    - sample_p(parms, n) functions samples F-values and computes p-values using the cumulative distribution function (CDF) of the F-distribution.\n",
    "3. Generates plots:\n",
    "    - QQ-Plot for F-values: here we comapre the theoretical quantiles (expected from F-distribution) vs. empirical (sorted simulated F-values). If the data points fall close to y=x, it suggests the simulated F follows the expected distribution.\n",
    "    - QQ-Plot for p-values: here we compare uniform theoritical values (0 to 1) to sorted simulated p-values.\n",
    "    - P-value Density Histogram: here we plot the histogram of p-values (excluding exacatly 1) with auto-bins as follows, if $ n \\geq 2 $ and IQR > 0:\n",
    "        - $\\text{bins} = \\left\\lceil \\frac{\\max(x) - \\min(x)}{2 \\cdot \\text{IQR} / n^{1/3}} \\right\\rceil$; where:\n",
    "            - If $ n < 2 $, returns 10\n",
    "            - If $ h $ is not finite or $ h \\leq 0 $, return 50.\n",
    "    - Empirical Cumulative Distribution Function (ECDF) of P-values: which plots the cumulative fraction of p-values vs. p-values to show how p-values accumulate and a uniform distribution indicate a good null calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667590f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "make_demo_from_calibration.py\n",
    "====================================\n",
    "Diagnostic plots for the single‑family calibration (flat vs 2‑PL).\n",
    "\n",
    "\"\"\"\n",
    "import json, warnings, re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.stats import f as f_dist, iqr\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "CAL_JSON = \"/projects/amp/asalehi/Dose/f_calibration/null_f_params_full_1M.json\"\n",
    "FIG_DIR  = \"/projects/amp/asalehi/Dose/f_calibration/figures\"\n",
    "N_DEMO   = 2_000_000\n",
    "MAX_BINS = 600\n",
    "\n",
    "Path(FIG_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ───────────────────────── colour palette ─────────────────────────\n",
    "# khaki‑like tone requested: #c3b091\n",
    "COLORS = {\"flat_vs_2pl\": \"#c3b091\"}\n",
    "\n",
    "# ───────────────────────── helper functions ───────────────────────\n",
    "def prettify(ax):\n",
    "    ax.set_facecolor(\"white\")\n",
    "    ax.yaxis.grid(True, ls=\"-\", lw=0.5, alpha=0.5, zorder=0)\n",
    "    ax.xaxis.grid(True, ls=\"-\", lw=0.5, alpha=0.5, zorder=0)\n",
    "    ax.set_axisbelow(True)\n",
    "    for spine in (\"top\", \"right\"):\n",
    "        ax.spines[spine].set_visible(False)\n",
    "    ax.xaxis.set_major_formatter(\n",
    "        ticker.FuncFormatter(lambda x, p: f\"{x:.2g}\" if x % 1 else f\"{x:.0f}\")\n",
    "    )\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\")\n",
    "\n",
    "def sample_F(params, n=N_DEMO):\n",
    "    df1, df2, loc, scale = params.values()\n",
    "    return loc + scale * f_dist.rvs(df1, df2, size=n)\n",
    "\n",
    "def sample_p(params, n=N_DEMO):\n",
    "    df1, df2, loc, scale = params.values()\n",
    "    F = loc + scale * f_dist.rvs(df1, df2, size=n)\n",
    "    return 1.0 - f_dist.cdf((F - loc) / scale, df1, df2)\n",
    "\n",
    "def freedman_bins(arr, *, max_bins=MAX_BINS):\n",
    "    arr = arr[np.isfinite(arr)]\n",
    "    if arr.size < 2:\n",
    "        return 10\n",
    "    h = 2 * iqr(arr) / np.cbrt(arr.size)\n",
    "    if not np.isfinite(h) or h <= 0:\n",
    "        return 50\n",
    "    return max(10, min(int(np.ceil(np.ptp(arr) / h)), max_bins))\n",
    "\n",
    "# ─────────────────────── load calibration ────────────────────────\n",
    "cal = json.loads(Path(CAL_JSON).read_text())\n",
    "\n",
    "# ───────────────────── per‑organism loop ─────────────────────────\n",
    "for org in sorted(k for k in cal if k != \"_meta\"):\n",
    "    print(f\"Processing organism: {org}\")\n",
    "    ctrl_blocks = {k: v for k, v in cal[org].items() if k in (\"growth\", \"sterility\")}\n",
    "    if not ctrl_blocks:\n",
    "        print(f\"No valid controls for {org}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    org_safe   = re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", org)\n",
    "    controls   = sorted(ctrl_blocks.keys())\n",
    "    n_controls = len(controls)\n",
    "\n",
    "    data = {}  # Initialize data dictionary for this organism\n",
    "\n",
    "    # Sample demo data for each control \n",
    "    for ctrl in controls:\n",
    "        params = ctrl_blocks[ctrl][\"flat_vs_2pl\"]\n",
    "        data[ctrl] = {\n",
    "            \"df1\":      params[\"df1\"],\n",
    "            \"df2\":      params[\"df2\"],\n",
    "            \"loc\":      params[\"loc\"],\n",
    "            \"scale\":    params[\"scale\"],\n",
    "            \"F_vals\":   sample_F(params),\n",
    "            \"p_vals\":   sample_p(params),\n",
    "            \"fam_label\":\"flat vs 2pl\",\n",
    "        }\n",
    "        data[ctrl][\"p_sorted\"] = np.sort(data[ctrl][\"p_vals\"])\n",
    "\n",
    "    # Figure\n",
    "    fig, axs = plt.subplots(\n",
    "        2, n_controls * 2,\n",
    "        figsize=(5 * n_controls * 2, 10),\n",
    "        dpi=800, facecolor=\"white\"\n",
    "    )\n",
    "    axs = np.atleast_2d(axs)  # ensure 2‑D\n",
    "\n",
    "    for col_idx, ctrl in enumerate(controls):\n",
    "        d = data[ctrl]\n",
    "        F_vals, p_vals, p_sorted = d[\"F_vals\"], d[\"p_vals\"], d[\"p_sorted\"]\n",
    "        df1, df2, loc, scale     = d[\"df1\"], d[\"df2\"], d[\"loc\"], d[\"scale\"]\n",
    "        fam_label = d[\"fam_label\"]\n",
    "        base = col_idx * 2  # first column for this control\n",
    "\n",
    "        # QQ‑plot (F, log‑log)\n",
    "        ax = axs[0, base]\n",
    "        prettify(ax)\n",
    "        n = len(F_vals)\n",
    "        q_theo = scale * f_dist.ppf((np.arange(1, n + 1) - 0.5) / n, df1, df2) + loc\n",
    "        q_emp  = np.sort(F_vals)\n",
    "        ax.scatter(q_theo, q_emp, s=10, color=COLORS[\"flat_vs_2pl\"], alpha=0.85)\n",
    "        ax.set_xscale(\"log\"); ax.set_yscale(\"log\")\n",
    "        lim = (min(q_emp.min(), q_theo.min()), max(q_emp.max(), q_theo.max()))\n",
    "        ax.plot(lim, lim, \"k--\", lw=1)\n",
    "        ax.set_xlim(lim); ax.set_ylim(lim)\n",
    "        ax.set_xlabel(\"Theoretical quantiles (log₁₀)\", fontsize=10, fontweight=\"bold\")\n",
    "        ax.set_ylabel(\"Empirical quantiles (log₁₀)\",  fontsize=10, fontweight=\"bold\")\n",
    "        ax.set_title(f\"{ctrl.title()} – QQ F\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "        # QQ‑plot (p)\n",
    "        ax = axs[0, base + 1]\n",
    "        prettify(ax)\n",
    "        u_theo = (np.arange(1, len(p_sorted) + 1) - 0.5) / len(p_sorted)\n",
    "        ax.scatter(u_theo, p_sorted, s=10, color=COLORS[\"flat_vs_2pl\"], alpha=0.85)\n",
    "        ax.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "        ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
    "        ax.set_xlabel(\"Theoretical U(0,1)\", fontsize=10, fontweight=\"bold\")\n",
    "        ax.set_ylabel(\"Empirical p\",        fontsize=10, fontweight=\"bold\")\n",
    "        ax.set_title(f\"{ctrl.title()} – QQ p\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "        # p‑value density\n",
    "        ax = axs[1, base]\n",
    "        prettify(ax)\n",
    "        bins = freedman_bins(p_vals[p_vals < 1])\n",
    "        ax.hist(\n",
    "            p_vals[p_vals < 1],\n",
    "            bins=bins, density=True,\n",
    "            color=COLORS[\"flat_vs_2pl\"], alpha=0.85, edgecolor=\"none\",\n",
    "            label=fam_label\n",
    "        )\n",
    "        ax.axhline(1, ls=\"--\", lw=1, color=\"grey\", label=\"ideal null\")\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_xlabel(\"p‑value\", fontsize=10, fontweight=\"bold\")\n",
    "        ax.set_ylabel(\"Density\",  fontsize=10, fontweight=\"bold\")\n",
    "        ax.set_title(f\"{ctrl.title()} – p‑value density\", fontsize=12, fontweight=\"bold\")\n",
    "        # legend with boxed, semi‑transparent background\n",
    "        leg = ax.legend(loc=\"lower left\",\n",
    "                        bbox_to_anchor=(0.02, 0.02),  # <- lower corner inside panel\n",
    "                        frameon=True,fontsize=8)\n",
    "        leg.get_frame().set_facecolor(\"white\")\n",
    "        leg.get_frame().set_alpha(0.95)\n",
    "\n",
    "        # ECDF of p‑values\n",
    "        ax = axs[1, base + 1]\n",
    "        prettify(ax)\n",
    "        y_ecdf = np.linspace(1 / len(p_sorted), 1, len(p_sorted))\n",
    "        ax.plot(p_sorted, y_ecdf, lw=5, color=COLORS[\"flat_vs_2pl\"], label=fam_label)\n",
    "        ax.plot([0, 1], [0, 1], \"k--\", lw=1, label=\"ideal\")\n",
    "        ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
    "        ax.set_xlabel(\"p‑value\", fontsize=10, fontweight=\"bold\")\n",
    "        ax.set_ylabel(\"ECDF\",    fontsize=10, fontweight=\"bold\")\n",
    "        ax.set_title(f\"{ctrl.title()} – ECDF\", fontsize=12, fontweight=\"bold\")\n",
    "        leg = ax.legend(loc=\"lower left\",\n",
    "                        bbox_to_anchor=(0.02, 0.02),  # <- lower corner inside panel\n",
    "                        frameon=True, fontsize=8)\n",
    "        leg.get_frame().set_facecolor(\"white\")\n",
    "        leg.get_frame().set_alpha(0.95)\n",
    "\n",
    "    # Final styling & save\n",
    "    fig.suptitle(\n",
    "        f\"{org} – Diagnostic plots (flat vs 2‑PL)\",\n",
    "        fontsize=14, fontweight=\"bold\", y=0.98\n",
    "    )\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95], pad=2.0)\n",
    "    fig.savefig(Path(FIG_DIR, f\"{org_safe}_diagnostics_2pl.png\"))\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"✓ generated combined facet plot for {org}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
